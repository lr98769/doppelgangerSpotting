---
title: "DMD Without ComBat"
author: "Wang Li Rong"
date: "1/1/2022"
output: html_document
---

# 1. Exploring doppelgangerIdentifier without ComBat
To explore the impacts of doppelganger identification and verification without any batch correction, we will be using the DMD data set.

## 0) Import packages

```{r Import & Install Packages, include=FALSE}
if (!("doppelgangerIdentifier" %in% installed.packages())){
  install.packages('devtools')
  library(devtools)
  install_github('lr98769/doppelgangerIdentifier')
}
library("doppelgangerIdentifier")

if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
if (!("biomaRt" %in% installed.packages())){
  BiocManager::install("biomaRt")
}
library("biomaRt")
```

Some functions for pre-processing and analysis:
```{r include = FALSE}
# This function converts the probes to ensemble id
# df = df with the column "Probe"
# 
convertProbesToEnsemble <- function(df, affy_attribute){
  if (!exists("ensembl")){
  ensembl <<- useEnsembl(biomart = "ensembl", 
                    dataset = 'hsapiens_gene_ensembl')
  }
  if (!"Probe" %in% colnames(df)){
    print("The column 'Probe' is missing from df")
    return()
  }
  return_list = list()
  mapping_table =  select(ensembl, 
                          keys= df$Probe, 
                          columns= c(affy_attribute,
                                     "ensembl_gene_id"),
                          keytype= affy_attribute)
  return_list[["mapping_table"]] = mapping_table
  # Ensure the each affy attribute maps to only 1 ensemble id 
  # (the smallest one) 
  # (Ensure that affy->ensemble is a one-to-one mapping)
  mapping_table = mapping_table[
    order(mapping_table[[affy_attribute]],
          mapping_table[["ensembl_gene_id"]]),]
  mapping_table = mapping_table[!duplicated(
    mapping_table[[affy_attribute]]),]
  
  # Remove all probes with no corresponding ensemble id
  df = df[df$Probe %in% mapping_table[[affy_attribute]],]
  # Replace rownames with the ensemble gene id
  rownames(mapping_table) = mapping_table[[affy_attribute]]
  df$ensembl_ID = mapping_table[df$Probe,"ensembl_gene_id"]
  return_list[["before_dedup"]] = df
  df$Probe = NULL
  # ensure order of variables stay the same for checking purposes
  current_order = unique(df$ensembl_ID)
  # Get the median signal of probes with the same ensemble id
  df = aggregate(df[,-ncol(df)], by = list(df[,"ensembl_ID"]) , median)
  rownames(df) = df$Group.1
  df$Group.1 = NULL
  df = df[current_order, ]
  return_list[["returned_df"]] = df
  return(return_list)
}
# Integrates file loading, replacing probes with ensemble id and caching of cleaned_data
getDataFile<- function(filename, affy_attribute, batch_name){
  cleaned_filename = paste("cleaned_",filename, sep="")
  if (!file.exists(cleaned_filename)){
    temp_df = read.csv(filename)
    temp_df$PROT = NULL # Remove PROT column
    # Convert probes to ensemble ids
    conversion_results <<- convertProbesToEnsemble(
                                df = temp_df,
                                affy_attribute = affy_attribute)
    temp_df = conversion_results$returned_df
    # Add H to the end so that we can differentiate it later
    colnames(temp_df) = paste(colnames(temp_df), batch_name, sep = "_")
    write.csv(temp_df, cleaned_filename)
    return(temp_df)
  } else {
    temp_df = read.csv(cleaned_filename, row.names = 1)
    return(temp_df)
  }
}
# Generates meta data table from an existing dataframe (Assumes each row is a different patient)
getMetaDataDataframe <-function(df, batch_name){
  meta = data.frame(row.names = colnames(df))
  meta$Patient_ID = rownames(meta)
  meta$Batch = batch_name
  meta$Class = substr(meta$Patient_ID, 1, 3)
  return(meta)
}
dfSetDifference <- function(df1, df2){
  setdifference = c()
  for (i in 1:nrow(df1)){
    if (!any(df2$Sample1 == df1[i, "Sample1"] & 
             df2$Sample2 == df1[i, "Sample2"])){
      setdifference = c(setdifference, i)
    }
  }
  return(df1[setdifference,])
}
getDoppelnNonDoppelSamples <- function(doppel_result, metadata, batchname){
  samples = unique(doppel_result$PPCC_df[doppel_result$PPCC_df$DoppelgangerLabel=="Doppelganger", "Sample1"])
  samples = c(samples, unique(doppel_result$PPCC_df[doppel_result$PPCC_df$DoppelgangerLabel=="Doppelganger", "Sample2"]))
  all_samples_of_batch = rownames(metadata[metadata$Batch==batchname,])
  doppel_samples = intersect(all_samples_of_batch, samples)
  non_doppel_samples = setdiff(all_samples_of_batch, doppel_samples)
  # Output in df form
  return_list = list(doppel_samples, non_doppel_samples)
  ## Compute maximum length
  max_length = max(sapply(return_list, length))
  ## Add NA values to list elements
  return_list = lapply(return_list, function(v) { 
    c(v, rep(NA, max_length-length(v)))
    })
  # Column bind them
  return_df = do.call(cbind, return_list)
  colnames(return_df) = c("Doppelganger Samples", "Non-Doppelganger Samples")
  return(return_df)
}

# Oversample the raw and meta data automatically
oversample_batch <- function(raw_data, meta_data, seed=2021){
  return_list = list()
  # get batch sizes
  batch_sizes = table(meta_data$Batch)
  # Get number of samples to over sample
  add_num_samples = max(batch_sizes) - min(batch_sizes)
  # Get batch to over sample
  min_batch = names(which.min(batch_sizes))
  # Get min_batch sample names
  min_batch_samples = rownames(meta_data[meta_data$Batch==min_batch, ])
  # Over sample some samples
  set.seed(seed)
  if (add_num_samples < length(min_batch_samples)){
    new_sample_names = sample(min_batch_samples, add_num_samples, 
                            replace = FALSE) 
  }
  else {
    print("Error: This function does not support oversampling of over twice the smaller batch's size")
    return()
  }
  
  new_samples_meta = meta_data[new_sample_names, ]
  rownames(new_samples_meta) = paste(rownames(new_samples_meta), "dup", sep="_")
  return_list[["meta_data"]] = as.data.frame(rbind(meta_data,new_samples_meta))
  
  new_samples_raw = raw_data[, new_sample_names]
  colnames(new_samples_raw) = paste(colnames(new_samples_raw), "dup", sep="_")
  return_list[["raw_data"]] = as.data.frame(cbind(raw_data,new_samples_raw))
  
  return(return_list)
}

# Removes added duplicate samples and sample pairs
remove_all_dup <-function(doppel_result){
  doppel_return = doppel_result
  # Get column names with "_dup"
  duplicates = grep('_dup', colnames(doppel_return$Batch_corrected), value=TRUE)
  doppel_return$Batch_corrected = doppel_return$Batch_corrected[, !(colnames(doppel_return$Batch_corrected) %in% duplicates)]
  doppel_return$PPCC_matrix = doppel_return$PPCC_matrix[, !(colnames(doppel_return$PPCC_matrix) %in% duplicates)]
  doppel_return$PPCC_matrix = doppel_return$PPCC_matrix[!(rownames(doppel_return$PPCC_matrix) %in% duplicates),]
  doppel_return$PPCC_df = doppel_return$PPCC_df[!(doppel_return$PPCC_df$Sample1 %in% duplicates) & !(doppel_return$PPCC_df$Sample2 %in% duplicates), ]
  return(doppel_return)
}

```

## 1) Import the DMD datasets

HaslettData
- 12 DMD Samples
- 12 Normal Samples

```{r}
# Converts the affy probes to ensemble id
dmd_h = getDataFile(filename= "data/DMD-HaslettData.csv",
                    affy_attribute = "affy_hg_u95av2",
                    batch_name = "H")
#Forming The metadata dataframe 
dmd_h_meta = getMetaDataDataframe(df = dmd_h, batch_name = "H")
```

PescatoriData
- 22 DMD Samples
- 14 Normal Samples

```{r}
dmd_p = getDataFile(filename = "data/DMD-PescatoriData.csv",
                    affy_attribute = "affy_hg_u133a",
                    batch_name = "P")

#Forming The metadata dataframe 
dmd_p_meta = getMetaDataDataframe(df = dmd_p, batch_name = "P")

```

## 2) Identify doppelgangers within and between both batches


### Doppelgangers Between Both Batches

Combining both data sets into one.  

```{r}
dmd_meta = data.frame(rbind(dmd_h_meta, dmd_p_meta))
#8813 variables in common
shared_variables_dmd = intersect(rownames(dmd_h), rownames(dmd_p)) 
dmd = data.frame(cbind(dmd_h[shared_variables_dmd, ], 
                       dmd_p[shared_variables_dmd,]))
```

Finding doppelgangers between both datasets

```{r}
doppel_hp_no_combat = getPPCCDoppelgangers(dmd, dmd_meta, 
                                           do_batch_corr = FALSE)
```
23 PPCC data Doppelgangers found between HaslettData and PescatoriData.

```{r}
table(doppel_hp_no_combat$PPCC_df$DoppelgangerLabel)
```

```{r}
visualisePPCCDoppelgangers(doppel_hp_no_combat)
```
The PPCC values are drastically lower without batch correction, and we also observe no negative PPCC values indicative of an outlier (unlike in DMD.Rmd where batch correction was employed). 


Output the doppelgangers for easier planning.

```{r}
if (!"openxlsx" %in% installed.packages()){
  install.packages("openxlsx")
}
library(openxlsx)
wb = createWorkbook()
addWorksheet(wb, "MetaData")
addWorksheet(wb, "DoppelgangerPairs")
addWorksheet(wb, "DoppelgangerSamplesH")
addWorksheet(wb, "DoppelgangerSamplesP")

writeData(wb, 1, dmd_meta)
writeData(wb, 2, 
          doppel_hp_no_combat$PPCC_df
          [doppel_hp_no_combat$PPCC_df$DoppelgangerLabel=="Doppelganger",])
writeData(wb, 3, getDoppelnNonDoppelSamples(
  doppel_result = doppel_hp_no_combat,
  metadata = dmd_meta,
  batchname = "H"))
writeData(wb, 4, getDoppelnNonDoppelSamples(
  doppel_result = doppel_hp_no_combat,
  metadata = dmd_meta,
  batchname = "P"))
saveWorkbook(wb, file = "output/dmd_no_combat_planning.xlsx", overwrite = TRUE)

```


## 4) Testing the Doppelganger Effect

We will now test if the PPCC data doppelgangers identified above (without batch correction) are still functional doppelgangers. 

**a) First, carry out functional doppelganger verification with batch correction prior to model training and validation. **

```{r}
veri_results_dmd_no_combat =  verifyDoppelgangers(
  "cleaned_data/dmd_no_combat_experiment_plan.csv",
  dmd,
  dmd_meta,
  do_batch_corr = TRUE,
  k=5,
  size_of_val_set = 6)
```

```{r fig.width=10, fig.height=5}
originalTrainValidNames =  c("Doppel_0", "Doppel_2","Doppel_4","Doppel_6", "Doppel_8","Doppel_10","Pos_Con", "Neg_Con")

newTrainValidNames =  c("0 Doppel", "2 Doppel", "4 Doppel", "6 Doppel", "8 Doppel","10 Doppel","Pos Con 10", "Neg Con")

visualiseVerificationResults(veri_results_dmd_no_combat,
                originalTrainValidNames,
                newTrainValidNames)
```

When the data set was batch corrected before training and validation, an inflationary effect can be observed. However, even in the 0 Doppel case, validation accuracy is already very high. This shows that not all functional doppelgangers between the training and validation set were identified when we do not use batch correction in PPCC data doppelganger identification. This demonstrates the importance of batch correction to the identification algorithm. 

**b) Next, we will observe the validation accuracies when no batch correction is done prior to model training and validation.  **

```{r}
veri_results_dmd_no_combat =  verifyDoppelgangers(
 "cleaned_data/dmd_no_combat_experiment_plan.csv",
  dmd,
  dmd_meta,
  do_batch_corr = FALSE,
  k=5,
  size_of_val_set = 6
 )
```

```{r fig.width=10, fig.height=5}
originalTrainValidNames =  c("Doppel_0", "Doppel_2","Doppel_4","Doppel_6", "Doppel_8","Doppel_10","Pos_Con", "Neg_Con")

newTrainValidNames =  c("0 Doppel", "2 Doppel", "4 Doppel", "6 Doppel", "8 Doppel","10 Doppel","Pos Con 10", "Neg Con")

visualiseVerificationResults(veri_results_dmd_no_combat,
                originalTrainValidNames,
                newTrainValidNames)
```
We observe no increase in accuracy as the number of PPCC data doppelgangers increased between the training and validation set. However, previously (when batch correlation was employed during verification) we observed the presence of the doppelganger effect. This shows the importance of the batch correction step for functional doppelganger verification. In the absence of batch correction, even functional doppelgangers do not display an inflationary effect. 

**c) Finally, we show the effect of no batch correction during verification on the PPCC Data doppelgangers identified with batch correction**

```{r}
veri_results_dmd_no_combat =  verifyDoppelgangers(
  "cleaned_data/dmd_experiment_plan.csv",
  dmd,
  dmd_meta,
  do_batch_corr = FALSE,
  k=5,
  size_of_val_set = 6
  )
```
```{r}
originalTrainValidNames =  c("Doppel_0", "Doppel_1","Doppel_2","Doppel_3", "Pos_Con_3", "Neg_Con")

newTrainValidNames =  c("0 Doppel", "1 Doppel", "2 Doppel", "3 Doppel", "3 Pos Con", "Neg Con")

visualiseVerificationResults(veri_results_dmd_no_combat,
                originalTrainValidNames,
                newTrainValidNames)
```
Once again, we observe no inflationary effects in the absence of batch correction during verification. This further emphasizes the importance of batch correction in the verification step. 
